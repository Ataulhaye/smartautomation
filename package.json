{
  "name": "smartautomation",
  "publisher": "Ata_Daniel",
  "displayName": "smartautomation",
  "description": "Automates the code comments and integration testing",
  "version": "0.0.1",
  "engines": {
    "vscode": "^1.96.2"
  },
  "categories": [
    "Other"
  ],
  "activationEvents": [],
  "main": "./dist/extension.js",
  "contributes": {
    "viewsContainers": {
      "activitybar": [
        {
          "id": "smartautomation",
          "title": "Smart Automation",
          "icon": "images/icon.svg",
          "visibleWhen": true
        }
      ]
    },
    "views": {
      "smartautomation": [
        {
          "id": "sideBar",
          "name": "",
          "type": "webview"
        }
      ]
    },
    "configuration": {
      "title": "Python Auto Commenter",
      "properties": {
        "Parameters.interval": {
          "type": "number",
          "default": 10000,
          "description": "Query interval of LLM Model in milliseconds"
        },
        "Parameters.LLMResponseValidation": {
          "type": "boolean",
          "default": "false",
          "description": "Enable LLM response validation, validation may take longer time"
        },
        "LLM.openAI.apiKey": {
          "type": "string",
          "default": "",
          "description": "LLM model Access Key"
        },
        "LLM.openAI.organizationId": {
          "type": "string",
          "default": "",
          "description": "Organization ID of OpenAI account"
        },
        "LLM.openAI.modelName": {
          "type": "string",
          "default": "gpt-4o",
          "description": "Name of the LLM model to use"
        },
        "LLM.openAI.max_completion_tokens": {
          "type": "integer",
          "default": 2048,
          "description": "Maximum tokens for model response"
        },
        "LLM.openAI.temperature": {
          "type": "number",
          "default": 0.3,
          "minimum": 0,
          "maximum": 1,
          "description": "Temperature parameter for the model"
        },
        "LLM.huggingFace.max_new_tokens": {
          "type": "integer",
          "default": 2048,
          "description": "Maximum tokens for model response"
        },
        "LLM.huggingFace.endpoint": {
          "type": "string",
          "default": "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct",
          "description": "API endpoint for the LLM model"
        },
        "LLM.huggingFace.apiKey": {
          "type": "string",
          "default": "",
          "description": "LLM model Access Key"
        },
        "LLM.huggingFace.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0,
          "maximum": 1,
          "description": "Temperature parameter for the model"
        },
        "LLM.ollama.modelName": {
          "type": "string",
          "default": "qwen2.5-coder",
          "description": "Name of the model, Ollama(https://ollama.com/) must be installed locally, and model must be downloaded"
        },
        "LLM.ollama.runLocalModel": {
          "type": "boolean",
          "default": false,
          "description": "To run the model locally, set to true"
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run package",
    "compile": "npm run check-types && npm run lint && node esbuild.js",
    "watch": "npm-run-all -p watch:*",
    "watch:esbuild": "node esbuild.js --watch",
    "watch:tsc": "tsc --noEmit --watch --project tsconfig.json",
    "package": "npm run check-types && npm run lint && node esbuild.js --production",
    "compile-tests": "tsc -p . --outDir out",
    "watch-tests": "tsc -p . -w --outDir out",
    "pretest": "npm run compile-tests && npm run compile && npm run lint",
    "check-types": "tsc --noEmit",
    "lint": "eslint src",
    "test": "vscode-test"
  },
  "devDependencies": {
    "@types/diff": "^7.0.0",
    "@types/jaro-winkler": "^0.2.3",
    "@types/mocha": "^10.0.9",
    "@types/node": "20.x",
    "@types/vscode": "^1.95.0",
    "@typescript-eslint/eslint-plugin": "^8.10.0",
    "@typescript-eslint/parser": "^8.7.0",
    "@vscode/test-cli": "^0.0.10",
    "@vscode/test-electron": "^2.4.1",
    "esbuild": "^0.24.0",
    "eslint": "^9.13.0",
    "npm-run-all": "^4.1.5",
    "typescript": "^5.6.3"
  },
  "dependencies": {
    "@azure/identity": "^4.5.0",
    "@azure/openai": "^2.0.0",
    "@types/node-fetch": "^2.6.12",
    "diff": "^7.0.0",
    "jaro-winkler": "^0.2.8",
    "node-fetch": "^2.7.0",
    "ollama": "^0.5.12",
    "openai": "^4.77.4"
  }
}
